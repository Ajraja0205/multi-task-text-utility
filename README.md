# M1 â€” FastAPI + Local LLM
Lightweight FastAPI service that demonstrates a local GenAI workflow using Ollama for LLM inference and ChromaDB for semantic memory.

Author: Ansh Jain
Assignment: GenAI â€“ M1
Tech Stack: Python, FastAPI, Ollama, ChromaDB

ğŸ“Œ Overview
This project is part of the M1 (Milestone 1) GenAI Assignment.

The objective is to build a local GenAI-powered text utility that runs entirely offline using:
ğŸ¦™ Ollama for local LLM inference
âš¡ FastAPI for REST APIs
ğŸ§  ChromaDB for semantic memory and caching
ğŸ Python for backend logic

The application accepts a user question and returns:
1.A generated answer
2.Confidence score
3.Latency
4.Estimated token usage
5.Estimated cost (0.0 for local LLM)

ğŸ¯ Objectives of M1
1.Run an LLM locally (no paid APIs)
2.Build a clean backend service using FastAPI
3.Separate concerns 6.(prompts, LLM client, memory, metrics)
4.Demonstrate prompt-based task handling
5.Track basic performance metrics
Implement semantic memory caching to reduce redundant LLM calls

ğŸ—ï¸ System Architecture
graph TD
    Client[Client / Postman / Swagger UI]

    Client -->|POST /ask| FastAPI[FastAPI App]

    FastAPI --> PromptBuilder[Prompt Builder]
    PromptBuilder --> LLMClient[LLM Client]

    LLMClient --> Memory[ChromaDB<br/>Semantic Memory]

    Memory -->|Cache Hit| FastAPI
    Memory -->|Cache Miss| Ollama[Ollama LLM<br/>phi3 Model]

    Ollama --> LLMClient
    LLMClient --> Metrics[Metrics Engine]
    Metrics --> FastAPI

    FastAPI --> Client

ğŸ” Request Flow (Sequence)
sequenceDiagram
    participant C as Client
    participant F as FastAPI
    participant M as ChromaDB
    participant L as Ollama LLM

    C->>F: POST /ask (question)
    F->>F: Build system prompt
    F->>M: Query semantic memory

    alt Similar question exists
        M-->>F: Cached answer
        F-->>C: Answer + metrics (cache)
    else No match
        F->>L: Generate response
        L-->>F: Raw LLM output
        F->>F: Normalize output
        F->>M: Store answer
        F-->>C: Answer + metrics (ollama)
    end

ğŸ§  Semantic Memory Decision Flow
flowchart TD
    Q[New Question]
    Q --> Search[Search ChromaDB]
    Search --> Check{Distance < Threshold?}

    Check -->|Yes| Cached[Return cached answer]
    Check -->|No| LLM[Call Ollama]

    LLM --> Clean[Normalize text]
    Clean --> Store[Store in ChromaDB]
    Store --> Return[Return answer]

ğŸ§± Project Structure
multi-task-text-utility/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py            # FastAPI entry point
â”‚   â”œâ”€â”€ llm_client.py      # Ollama LLM + caching logic
â”‚   â”œâ”€â”€ memory.py          # ChromaDB semantic memory
â”‚   â”œâ”€â”€ metrics.py         # Latency, token & cost estimation
â”‚   â”œâ”€â”€ prompts.py         # Prompt templates
â”‚
â”œâ”€â”€ chroma_db/             # Persistent vector storage
â”œâ”€â”€ screenshots/           # API & Swagger screenshots
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ .env.example
â””â”€â”€ README.md

âš™ï¸ Prerequisites

Ensure the following are installed:
Python 3.10+
Ollama
Git (optional)
Postman / Browser

ğŸ¦™ Ollama Setup (Local LLM)
1ï¸âƒ£ Verify installation
ollama --version
ollama pull phi3
(Other supported models: llama3, mistral)

2ï¸âƒ£ Test model
ollama run phi3

ğŸ Python Environment Setup
1ï¸âƒ£ Create virtual environment
python -m venv venv

2ï¸âƒ£ Activate environment
Windows : venv\Scripts\Activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

â–¶ï¸ Running the Application
uvicorn app.main:app --reload

App will be available at: http://127.0.0.1:8000

ğŸŒ API Endpoints
âœ… Health Check

GET /
Response: { "status": "ok" }

ğŸ¤– Ask Question

POST /ask

Request:
{
  "question": "My internet is not working, what should I do?"
}

Response:
{
  "answer": "You can try restarting your router...",
  "confidence": 0.75,
  "metrics": {
    "latency": 1.23,
    "tokens": 132,
    "estimated_cost": 0.0
  }
}

ğŸ“Š Metrics Explanation
Latency â€“ Response time (seconds)
Tokens (Estimated) â€“ Approximate token count
Estimated Cost â€“ Always 0.0 (local inference)

ğŸ§  Prompt Design
Prompts are stored in prompts.py: 
CUSTOMER_SUPPORT_PROMPT = """
You are a helpful AI assistant.
Answer clearly in plain English.
Do not return JSON or code blocks.
"""

This ensures:
Prompt reusability
Clean separation of concerns
Easy extension to future tasks

ğŸ’¾ Semantic Memory (ChromaDB)
Answers are stored as vector embeddings
Similar questions are reused automatically
Distance threshold prevents incorrect reuse
Persistent across restarts

This improves:
Latency
Cost efficiency
Answer consistency

ğŸ“ Environment Variables

.env.example

LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=phi3
LLM_TIMEOUT=60

ğŸ§ª API Documentation (Swagger)

FastAPI auto-docs: http://localhost:8000/docs

ğŸš€ Future Enhancements (M2+)
Multi-task routing (summarization, classification)
Streaming responses
Dockerization
Authentication
RAG with document ingestion
Advanced logging & observability

âœ… M1 Checklist

âœ” Local LLM via Ollama
âœ” FastAPI backend
âœ” Prompt separation
âœ” Metrics captured
âœ” Semantic memory (ChromaDB)
âœ” Fully local & offline
âœ” Evaluation-ready

ğŸ‘¤ Author

Ansh Jain
GenAI â€“ M1 Assignment
Focused on scalable LLM systems & clean backend architecture

ğŸ“Œ Notes
No paid APIs used
Fully local execution
Suitable for offline development & evaluation

âœ… M1 Assignment Completed Successfully
âœ… Extended with semantic memory and caching