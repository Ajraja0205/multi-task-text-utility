# ğŸ“˜ M1 Assignment â€“ Multi-Task Text Utility (Local LLM using Ollama)

## ğŸ“Œ Overview

This project is part of the **M1 (Milestone 1) GenAI Assignment**. The goal is to build a **local GenAI-powered text utility** using:

* **Ollama** for running Large Language Models locally
* **FastAPI** for exposing REST APIs
* **Python** for backend logic

The application exposes an API endpoint that accepts a user question, sends it to a locally running LLM, and returns:

* The generated answer
* Basic metrics such as latency, token estimation, and cost estimation

---

## ğŸ¯ Objectives of M1

* Run an LLM **locally** (no OpenAI / paid APIs)
* Build a clean backend service using FastAPI
* Separate concerns: prompts, LLM client, metrics
* Demonstrate prompt-based task handling
* Track basic performance metrics

---

## ğŸ§± Project Structure

```
multi-task-text-utility/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py            # FastAPI entry point
â”‚   â”œâ”€â”€ llm_client.py      # Ollama LLM integration
â”‚   â”œâ”€â”€ prompts.py         # Prompt templates
â”‚   â”œâ”€â”€ metrics.py         # Latency, token & cost estimation
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ venv/                  # Python virtual environment
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # Project documentation
â””â”€â”€ .env (optional)        # Environment variables (if needed)
```

---

## âš™ï¸ Prerequisites

Make sure the following are installed on your system:

* **Python 3.10+** (recommended: 3.11 or 3.12)
* **Ollama** (installed and added to PATH)
* **Git** (optional)
* **Postman** or browser (for API testing)

---

## ğŸ¦™ Ollama Setup (Local LLM)

### 1ï¸âƒ£ Verify Ollama installation

```bash
ollama --version
```

If not installed, download from:
[https://ollama.com](https://ollama.com)

---

### 2ï¸âƒ£ Pull a model (example: llama3)

```bash
ollama pull llama3
```

You can also use:

* `mistral`
* `phi3`

---

### 3ï¸âƒ£ Test model locally

```bash
ollama run llama3
```

If the model responds, Ollama is working correctly.

---

## ğŸ Python Environment Setup

### 1ï¸âƒ£ Create virtual environment

```bash
python -m venv venv
```

### 2ï¸âƒ£ Activate virtual environment

**Windows (PowerShell):**

```bash
venv\Scripts\Activate.ps1
```

---

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Running the Application

From the project root directory:

```bash
uvicorn app.main:app --reload
```

Expected output:

```
Uvicorn running on http://127.0.0.1:8000
```

---

## ğŸŒ API Endpoints

### âœ… Health Check

**GET** `/`

```
http://localhost:8000/
```

Response:

```json
{
  "status": "ok"
}
```

---

### ğŸ¤– Ask Question (Main Endpoint)

**POST** `/ask`

```
http://localhost:8000/ask
```

#### Headers

```
Content-Type: application/json
```

#### Request Body

```json
{
  "question": "My internet is not working, what should I do?"
}
```

#### Sample Response

```json
{
  "answer": "You can try restarting your router...",
  "metrics": {
    "latency": 1.74,
    "tokens": 128,
    "estimated_cost": 0.0
  }
}
```

---

## ğŸ“Š Metrics Explanation

The following metrics are captured:

* **Latency** â€“ Time taken for the LLM to respond (in seconds)
* **Tokens (Estimated)** â€“ Rough token count based on text length
* **Estimated Cost** â€“ Always `0.0` since Ollama runs locally

---

## ğŸ§  Prompt Design

Prompts are stored in `prompts.py` and injected dynamically.

Example:

```python
CUSTOMER_SUPPORT_PROMPT = """
You are a helpful customer support assistant.
Answer politely and clearly.
"""
```

This ensures:

* Reusability
* Clean separation of logic
* Easy extension for future tasks

---

## ğŸ“ Environment Variables (.env)

For M1, `.env` is **optional**.

If used, it may contain:

```
OLLAMA_MODEL=llama3
OLLAMA_BASE_URL=http://localhost:11434
```

---

## ğŸ§ª API Documentation (Swagger UI)

FastAPI provides automatic API docs:

```
http://localhost:8000/docs
```

This can be used for quick testing and screenshots for submission.

---

## ğŸš€ Future Enhancements (M2+)

* Multiple task routing (summarization, classification, rewrite)
* Streaming responses
* Authentication
* Dockerization
* Advanced metrics & logging

---

## âœ… M1 Checklist

âœ” Local LLM via Ollama
âœ” FastAPI backend
âœ” Clean project structure
âœ” Prompt separation
âœ” Metrics captured
âœ” Postman & Swagger tested

---

## ğŸ‘¤ Author

**Name:** Ansh Jain
**Assignment:** GenAI â€“ M1
**Tech Stack:** Python, FastAPI, Ollama, LLMs

---

## ğŸ“Œ Notes

* No paid APIs used
* Entire project runs locally
* Suitable for offline development

---

âœ… **M1 Assignment Completed Successfully**
âœ… *Project is complete, functional, and evaluation-ready.*
